{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POT (Python Optimal Transport) package is not installed. Try to run $ conda install -c conda-forge pot ; or $ pip install POT\n",
      "Rips(maxdim=1, thresh=inf, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transforms import *\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ripser import lower_star_img\n",
    "from ripser import Rips\n",
    "vr = Rips()\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "import persim\n",
    "import diagram2vec\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from gtda.diagrams import PersistenceEntropy, PersistenceImage, BettiCurve\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 300\n",
    "sigma1 = 4\n",
    "sigma2 = 2\n",
    "t = 0.01\n",
    "\n",
    "def generate(N, S, W=300, sigma1=4, sigma2=2, t=0.01, bins=64):\n",
    "\n",
    "    z = np.zeros((N, S, 2))\n",
    "    for n in range(N):\n",
    "        z[n, 0] = np.random.uniform(0, W, size=(2))\n",
    "        for s in range(S-1):\n",
    "            d_1 = np.random.normal(0, sigma1)\n",
    "            d_2 = np.random.normal(0, sigma1)\n",
    "            z[n, s+1, 0] = (z[n, s, 0] + d_1) % W\n",
    "            z[n, s+1, 1] = (z[n, s, 1] + d_2) % W\n",
    "\n",
    "    z_r = z.reshape(N*S, 2)\n",
    "    H, _, _ = np.histogram2d(z_r[:,0], z_r[:,1], bins=bins)\n",
    "    \n",
    "    G = gaussian_filter(H, sigma2)\n",
    "    G[G < t] = 0\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:35<00:00, 280.81it/s]\n",
      "100%|██████████| 10000/10000 [00:29<00:00, 333.62it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "classes_count = 2\n",
    "\n",
    "images = np.zeros((classes_count * count, 64, 64))\n",
    "\n",
    "# class A\n",
    "N = 100\n",
    "S = 30\n",
    "\n",
    "for n in tqdm(range(count)):\n",
    "    images[n] = generate(N, S)\n",
    "    \n",
    "# class B\n",
    "N = 250\n",
    "S = 10\n",
    "\n",
    "for n in tqdm(range(count)):\n",
    "    images[n+count] = generate(N, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_diagrams(diagrams, name):\n",
    "    labels = [0 for _ in range(count)] + [1 for _ in range(count)]\n",
    "    pairs = list(zip(diagrams, labels))\n",
    "\n",
    "    train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_diagrams, train_labels = zip(*train_pairs)\n",
    "    test_diagrams, test_labels = zip(*test_pairs)\n",
    "\n",
    "    train_dataset = DiagramsDataset(train_diagrams, train_labels)\n",
    "    test_dataset = DiagramsDataset(test_diagrams, test_labels)\n",
    "\n",
    "    torch.save(train_dataset, f\"../data/porus/{name}_train.pt\")\n",
    "    torch.save(test_dataset, f\"../data/porus/{name}_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:04<00:00, 161.23it/s]\n"
     ]
    }
   ],
   "source": [
    "baseline_diagrams = [\n",
    "    process_baseline(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(images)\n",
    "]\n",
    "push_diagrams(baseline_diagrams, \"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:10<00:00, 153.14it/s]\n"
     ]
    }
   ],
   "source": [
    "cedt_diagrams = [\n",
    "    process_cedt(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(images)\n",
    "]\n",
    "push_diagrams(cedt_diagrams, \"cedt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:12<00:00, 150.99it/s]\n",
      "100%|██████████| 20000/20000 [02:12<00:00, 151.21it/s]\n",
      "100%|██████████| 20000/20000 [02:12<00:00, 150.83it/s]\n",
      "100%|██████████| 20000/20000 [02:13<00:00, 149.59it/s]\n",
      "100%|██████████| 20000/20000 [02:13<00:00, 150.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for thickening in [1, 2, 3, 5, 8]:\n",
    "    cedt_diagrams = [\n",
    "        process_cedt_thickening(torch.Tensor(t).flatten(), thickening, device=\"cpu\") for t in tqdm(images)\n",
    "    ]\n",
    "    push_diagrams(cedt_diagrams, f\"cedt_thickening_{thickening}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:06<00:00, 157.72it/s]\n",
      "100%|██████████| 20000/20000 [08:25<00:00, 39.58it/s]\n",
      "100%|██████████| 20000/20000 [20:49<00:00, 16.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for dir_count in [1, 4, 10]:\n",
    "    dirs = np.arange(dir_count) / dir_count * 2 * np.pi\n",
    "    cedt_diagrams = [\n",
    "        process_image(torch.Tensor(t).flatten(), dirs, device=\"cpu\") for t in tqdm(images)\n",
    "    ]\n",
    "    push_diagrams(cedt_diagrams, f\"directional_{dir_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:05<00:00, 159.43it/s]\n",
      "100%|██████████| 20000/20000 [08:35<00:00, 38.82it/s]\n",
      "100%|██████████| 20000/20000 [20:44<00:00, 16.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for conv_count in [1, 4, 10]:\n",
    "    conv = nn.Conv2d(1, conv_count, kernel_size=3)\n",
    "    cedt_diagrams = [\n",
    "        process_by_conv(torch.Tensor(t).flatten(), conv, device=\"cpu\") for t in tqdm(images)\n",
    "    ]\n",
    "    push_diagrams(cedt_diagrams, f\"convolution_{conv_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0 for _ in range(count)] + [1 for _ in range(count)]\n",
    "pairs = list(zip(images, labels))\n",
    "\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "train_images, train_labels = zip(*train_pairs)\n",
    "test_images, test_labels = zip(*test_pairs)\n",
    "\n",
    "train_dataset = ImagesDataset(train_images, train_labels)\n",
    "test_dataset = ImagesDataset(test_images, test_labels)\n",
    "\n",
    "torch.save(train_dataset, f\"../data/porus/images_train.pt\")\n",
    "torch.save(test_dataset, f\"../data/porus/images_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "mnist_train = torchvision.datasets.MNIST('../data/mnist/base_mnist_train_data', train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.MNIST('../data/mnist/base_mnist_test_data', train=False, download=True, transform=transform)\n",
    "X_train = mnist_train.data.numpy()\n",
    "X_test = mnist_test.data.numpy()\n",
    "y_train = mnist_train.targets.numpy()\n",
    "y_test = mnist_test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_diagrams_mnist(train_diagrams, test_diagrams, train_labels, test_labels, name):\n",
    "    train_dataset = DiagramsDataset(train_diagrams, train_labels)\n",
    "    test_dataset = DiagramsDataset(test_diagrams, test_labels)\n",
    "\n",
    "    torch.save(train_dataset, f\"../data/mnist/{name}_train.pt\")\n",
    "    torch.save(test_dataset, f\"../data/mnist/{name}_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:16<00:00, 779.34it/s]\n",
      "100%|██████████| 10000/10000 [00:12<00:00, 782.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_diagrams = [\n",
    "    process_baseline(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_train)\n",
    "]\n",
    "test_diagrams = [\n",
    "    process_baseline(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_test)\n",
    "]\n",
    "push_diagrams_mnist(train_diagrams, test_diagrams, y_train, y_test, \"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:20<00:00, 743.83it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 750.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train_diagrams = [\n",
    "    process_cedt(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_train)\n",
    "]\n",
    "test_diagrams = [\n",
    "    process_cedt(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_test)\n",
    "]\n",
    "push_diagrams_mnist(train_diagrams, test_diagrams, y_train, y_test, \"cedt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:23<00:00, 722.19it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 717.80it/s]\n",
      "100%|██████████| 60000/60000 [01:23<00:00, 716.05it/s]\n",
      "100%|██████████| 10000/10000 [00:13<00:00, 725.56it/s]\n",
      "100%|██████████| 60000/60000 [01:27<00:00, 685.53it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 710.15it/s]\n",
      "100%|██████████| 60000/60000 [01:26<00:00, 694.54it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 682.70it/s]\n",
      "100%|██████████| 60000/60000 [01:26<00:00, 696.39it/s]\n",
      " 81%|████████  | 8058/10000 [00:11<00:02, 698.95it/s]"
     ]
    }
   ],
   "source": [
    "for thickening in [1, 2, 3, 5, 8]:\n",
    "    train_diagrams = [\n",
    "        process_cedt_thickening(torch.Tensor(t).flatten(), thickening, device=\"cpu\") for t in tqdm(X_train)\n",
    "    ]\n",
    "    test_diagrams = [\n",
    "        process_cedt_thickening(torch.Tensor(t).flatten(), thickening, device=\"cpu\") for t in tqdm(X_test)\n",
    "    ]\n",
    "    push_diagrams_mnist(train_diagrams, test_diagrams, y_train, y_test, f\"cedt_thickening_{thickening}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_count in [1, 4, 10]:\n",
    "    dirs = np.arange(dir_count) / dir_count * 2 * np.pi\n",
    "    train_diagrams = [\n",
    "        process_image(torch.Tensor(t).flatten(), dirs, device=\"cpu\") for t in tqdm(X_train)\n",
    "    ]\n",
    "    test_diagrams = [\n",
    "        process_image(torch.Tensor(t).flatten(), dirs, device=\"cpu\") for t in tqdm(X_test)\n",
    "    ]\n",
    "    push_diagrams_mnist(train_diagrams, test_diagrams, y_train, y_test, f\"directional_{dir_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv_count in [1, 4, 10]:\n",
    "    torch.random.manual_seed(42)\n",
    "    conv = nn.Conv2d(1, conv_count, kernel_size=3)\n",
    "    train_diagrams = [\n",
    "        process_by_conv(torch.Tensor(t).flatten(), conv, device=\"cpu\") for t in tqdm(X_train)\n",
    "    ]\n",
    "    test_diagrams = [\n",
    "        process_by_conv(torch.Tensor(t).flatten(), conv, device=\"cpu\") for t in tqdm(X_test)\n",
    "    ]\n",
    "    push_diagrams(cedt_diagrams, f\"convolution_{conv_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImagesDataset(X_train, y_train)\n",
    "test_dataset = ImagesDataset(X_test, y_test)\n",
    "\n",
    "torch.save(train_dataset, f\"../data/mnist/images_train.pt\")\n",
    "torch.save(test_dataset, f\"../data/mnist/images_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "cifar_train = torchvision.datasets.CIFAR10('../data/cifar10/base_cifar10_train_data', train=True, download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10('../data/cifar10/base_cifar10_test_data', train=False, download=True, transform=transform)\n",
    "X_train = cifar_train.data.numpy()\n",
    "X_test = cifar_test.data.numpy()\n",
    "y_train = np.array(cifar_train.targets)\n",
    "y_test = np.array(cifar_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_diagrams_cifar(train_diagrams, test_diagrams, train_labels, test_labels, name):\n",
    "    train_dataset = DiagramsDataset(train_diagrams, train_labels)\n",
    "    test_dataset = DiagramsDataset(test_diagrams, test_labels)\n",
    "\n",
    "    torch.save(train_dataset, f\"../data/cifar10/{name}_train.pt\")\n",
    "    torch.save(test_dataset, f\"../data/cifar10/{name}_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diagrams = [\n",
    "    process_baseline(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_train)\n",
    "]\n",
    "test_diagrams = [\n",
    "    process_baseline(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_test)\n",
    "]\n",
    "push_diagrams_cifar(train_diagrams, test_diagrams, y_train, y_test, \"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diagrams = [\n",
    "    process_cedt(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_train)\n",
    "]\n",
    "test_diagrams = [\n",
    "    process_cedt(torch.Tensor(t).flatten(), device=\"cpu\") for t in tqdm(X_test)\n",
    "]\n",
    "push_diagrams_cifar(train_diagrams, test_diagrams, y_train, y_test, \"cedt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thickening in [1, 2, 3, 5, 8]:\n",
    "    train_diagrams = [\n",
    "        process_cedt_thickening(torch.Tensor(t).flatten(), thickening, device=\"cpu\") for t in tqdm(X_train)\n",
    "    ]\n",
    "    test_diagrams = [\n",
    "        process_cedt_thickening(torch.Tensor(t).flatten(), thickening, device=\"cpu\") for t in tqdm(X_test)\n",
    "    ]\n",
    "    push_diagrams_cifar(train_diagrams, test_diagrams, y_train, y_test, f\"cedt_thickening_{thickening}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_count in [1, 4, 10]:\n",
    "    dirs = np.arange(dir_count) / dir_count * 2 * np.pi\n",
    "    train_diagrams = [\n",
    "        process_image(torch.Tensor(t).flatten(), dirs, device=\"cpu\") for t in tqdm(X_train)\n",
    "    ]\n",
    "    test_diagrams = [\n",
    "        process_image(torch.Tensor(t).flatten(), dirs, device=\"cpu\") for t in tqdm(X_test)\n",
    "    ]\n",
    "    push_diagrams_cifar(train_diagrams, test_diagrams, y_train, y_test, f\"directional_{dir_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImagesDataset(X_train, y_train)\n",
    "test_dataset = ImagesDataset(X_test, y_test)\n",
    "\n",
    "torch.save(train_dataset, f\"../data/cifar10/images_train.pt\")\n",
    "torch.save(test_dataset, f\"../data/cifar10/images_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "cifar_train = torchvision.datasets.CIFAR10('../data/cifar10/base_cifar10_train_data', train=True, download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10('../data/cifar10/base_cifar10_test_data', train=False, download=True, transform=transform)\n",
    "X_train = cifar_train.data.numpy()\n",
    "X_test = cifar_test.data.numpy()\n",
    "y_train = np.array(cifar_train.targets)\n",
    "y_test = np.array(cifar_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
